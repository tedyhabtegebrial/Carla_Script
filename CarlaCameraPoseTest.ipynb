{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def carla_k_matrix(fov=90.0, height=600, width=800):\n",
    "    k = np.identity(3)\n",
    "    k[0, 2] = width / 2.0\n",
    "    k[1, 2] = height / 2.0\n",
    "    k[0, 0] = k[1, 1] = width / \\\n",
    "                        (2.0 * math.tan(fov * math.pi / 360.0))\n",
    "    return torch.from_numpy(k)\n",
    "\n",
    "def read_depth(frame_num):\n",
    "    input_path = os.path.abspath(f'./examples2/ForwardCamera0Depth/{str(frame_num).zfill(6)}.png')\n",
    "    img = np.asarray(Image.open(input_path), dtype=np.uint8)\n",
    "    img = img.astype(np.float64)  # .double()\n",
    "    normalized_depth = np.dot(img, [1.0, 256.0, 65536.0])\n",
    "    normalized_depth /= 16777215.0  # (256.0 * 256.0 * 256.0 - 1.0)\n",
    "    #normalized_depth = torch.from_numpy(normalized_depth * 1000.0)\n",
    "    normalized_depth = torch.from_numpy(normalized_depth * 1000.0)\n",
    "    return normalized_depth.float()\n",
    "\n",
    "def read_img(frame_num):\n",
    "    input_path = os.path.abspath(f'./examples2/ForwardCamera0RGB/{str(frame_num).zfill(6)}.png')\n",
    "    img = np.asarray(Image.open(input_path))\n",
    "    return torch.from_numpy(img).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load poses of two cameras, their depth and color images then warpe on to the other\n",
    "def read_camera_poses(frame_num):\n",
    "    rotations, translations = [], []\n",
    "    with open('/home/habtegebrial/Desktop/Work/repos/PythonLibs/Carla_Script/examples2/ForwardCamera0.txt', 'r') as fid:\n",
    "        for line in fid:\n",
    "            data = np.array([float(p) for p in line.split(' ')]).reshape(3, 4)\n",
    "            rotations.append(data[:, 0:3])\n",
    "            translations.append(data[:, 3])\n",
    "    rotations, translations =  np.array(rotations), np.array(translations)\n",
    "    rotations, translations =  torch.from_numpy(rotations), torch.from_numpy(translations)\n",
    "    return rotations[frame_num].view(3,3).float(), translations[frame_num].view(3, 1).float()\n",
    "\n",
    "id_0, id_1 = 45, 46\n",
    "depth_0, depth_1 = read_depth(id_0), read_depth(id_1)\n",
    "\n",
    "img_0, img_1 = read_img(id_0).permute(2, 0, 1)/255.0, read_img(id_1).permute(2, 0, 1)/255.0\n",
    "R_0, T_0 = read_camera_poses(id_0)\n",
    "R_1, T_1 = read_camera_poses(id_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 600, 800])\n",
      "Save a given Tensor into an image file.\n",
      "\n",
      "    Args:\n",
      "        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n",
      "            saves the tensor as a grid of images by calling ``make_grid``.\n",
      "        **kwargs: Other arguments are documented in ``make_grid``.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# compute camera 1 from camera 0\n",
    "_, h, w = img_0.shape\n",
    "print(img_0.shape)\n",
    "x_locs = torch.linspace(0, w-1, w).view(1, w, 1).expand(h, w, 1)\n",
    "y_locs = torch.linspace(0, h-1, h).view(h, 1, 1).expand(h, w, 1)\n",
    "ones = torch.ones_like(x_locs)\n",
    "\n",
    "hom_pts = torch.cat([x_locs, y_locs, ones], 2).view(h*w, 3, 1)\n",
    "\n",
    "k_matrix = carla_k_matrix().float().view(1, 3, 3).expand(h*w, 3, 3)\n",
    "k_inv = torch.inverse(carla_k_matrix().float()).view(1, 3, 3).expand(h*w, 3, 3)\n",
    "\n",
    "# unproject\n",
    "depth_vals = depth_1.view(h, w, 1).expand(h, w, 3).view(-1, 3, 1).float()\n",
    "pts_3d = depth_vals * torch.bmm(k_inv, hom_pts).float()\n",
    "\n",
    "# rotate from cam 1 to world\n",
    "pts_3d_world = torch.bmm(R_1.view(-1, 3, 3).expand(h*w, 3, 3),\n",
    "                        pts_3d) + T_1.view(-1, 3, 1).expand(h*w, 3, 1)\n",
    "\n",
    "# world to camera 0\n",
    "rot_w_0 = torch.inverse(R_0).view(-1, 3, 3).expand(h*w, 3, 3)\n",
    "t_vec_w_0 = torch.mm(torch.inverse(R_0), (-1*T_0.view(3, 1)))\n",
    "t_vec_w_0 = t_vec_w_0.view(-1, 3, 1).expand(h*w, 3, 1)\n",
    "pts_3d_cam_0 = torch.bmm(rot_w_0,\n",
    "                        pts_3d_world) + t_vec_w_0\n",
    "\n",
    "pt3_cam_0_hom = torch.bmm(k_matrix, pts_3d_cam_0).view(-1, 3)\n",
    "\n",
    "pt3_cam_0_hom[..., 0] /= pt3_cam_0_hom[..., 2]\n",
    "pt3_cam_0_hom[..., 1] /= pt3_cam_0_hom[..., 2]\n",
    "\n",
    "grid_pts_int = pt3_cam_0_hom[..., :-1]\n",
    "grid_pts_int = grid_pts_int.view(h, w, 2)\n",
    "h_w, h_h = (w-1)/2.0, (h-1)/2.0\n",
    "grid_pts_int[..., 0] = (grid_pts_int[..., 0]-h_w)/h_w\n",
    "grid_pts_int[..., 1] = (grid_pts_int[..., 1]-h_h)/h_h\n",
    "\n",
    "# \n",
    "grid_pts_float = grid_pts_int.view(1, h, w, 2)\n",
    "# input_img = img_0.view(1, 3, h, w)\n",
    "\n",
    "warped_img = F.grid_sample(input=img_0.view(1, 3, h, w), grid=grid_pts_float)\n",
    "print(torchvision.utils.save_image.__doc__)\n",
    "torchvision.utils.save_image(filename='warped_img.png',\n",
    "                             tensor=torchvision.utils.make_grid(warped_img))\n",
    "torchvision.utils.save_image(filename='original.png', tensor=img_1)\n",
    "\n",
    "# print(grid_pts_int.shape)\n",
    "# print(img_1.max())\n",
    "# print(warped_img.max())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[381.7031],\n",
       "        [326.2776],\n",
       "        [ 39.6090]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[368.8245],\n",
       "        [326.2834],\n",
       "        [ 39.5780]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3.6",
   "language": "python",
   "name": "py_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
